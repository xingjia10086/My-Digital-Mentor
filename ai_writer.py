import os
import argparse
from dotenv import load_dotenv
from google import genai
from langchain_google_vertexai import VertexAIEmbeddings
from langchain_community.vectorstores import Chroma

load_dotenv()

# --- Configuration ---
PROJECT_ID = os.environ.get("GCP_PROJECT_ID", "gen-lang-client-0834352502")
LOCATION = os.environ.get("GCP_LOCATION", "us-central1")
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
CHROMA_PERSIST_DIR = os.path.join(BASE_DIR, "chroma_db")
EMBEDDING_MODEL = "text-embedding-004"
API_KEY = os.environ.get("GOOGLE_API_KEY", "")
client = genai.Client(api_key=API_KEY)


def format_docs(docs):
    formatted = []
    for d in docs:
        source = d.metadata.get('source_file', 'æœªçŸ¥æ–‡ç« ')
        formatted.append(f"ã€å‚è€ƒç¯‡ç›®ï¼šã€Š{source}ã€‹ã€‘\n{d.page_content}")
    return "\n\n---\n\n".join(formatted)


def generate_draft(topic):
    print("=== æ˜Ÿä½³çš„æ•°å­—åˆ†èº«ï¼šæ–‡ç« èµ·è‰æœº ===")
    print(f"\n[ä»»åŠ¡ç›®æ ‡]: æ„æ€å…³äºâ€œ{topic}â€çš„æ–‡ç« \n")
    
    if not os.path.exists(CHROMA_PERSIST_DIR):
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°çŸ¥è¯†åº“ {CHROMA_PERSIST_DIR}")
        return
        
    print("[1/3] æ­£åœ¨é€‰æ‹©æœ€ä½³æ¨¡å‹...")
    available = []
    for m in client.models.list():
        if 'gemini' in m.name.lower() and 'generate' in str(getattr(m, 'supported_actions', '')).lower():
            available.append(m.name)
            
    chosen_model = "gemini-2.0-flash"
    for preferred in ["gemini-2.5-pro", "gemini-2.0-flash", "gemini-1.5-pro"]:
        for a in available:
            if preferred in a:
                chosen_model = a
                break
        if chosen_model != "gemini-2.0-flash":
            break
            
    print(f"  âœ“ ä½¿ç”¨æ¨¡å‹: {chosen_model}")
    
    print("\n[2/3] æ­£åœ¨ä» 1.8ä¸‡ ä¸ªå†å²ç‰‡æ®µä¸­æ£€ç´¢ç›¸å…³çµæ„Ÿä¸é‡‘å¥...")
    embeddings = VertexAIEmbeddings(
        model_name=EMBEDDING_MODEL,
        project=PROJECT_ID,
        location=LOCATION
    )
    vectorstore = Chroma(
        persist_directory=CHROMA_PERSIST_DIR,
        embedding_function=embeddings,
        collection_name="wechat_articles"
    )
    
    # We want more context for writing than just casual conversation
    retriever = vectorstore.as_retriever(search_kwargs={"k": 8})
    docs = retriever.invoke(topic)
    context_str = format_docs(docs)

    unique_sources = []
    for d in docs:
        src = d.metadata.get('source_file', 'æœªçŸ¥')
        if src not in unique_sources:
            unique_sources.append(src)
            print(f"  ğŸ“– æ¿€æ´»è®°å¿†èŠ‚ç‚¹: ã€Š{src}ã€‹")
            
    print("\n[3/3] å¼€å§‹æç¬”æ’°å†™åˆç¨¿...\n")
    print("=" * 60)
    
    prompt = f"""ä½ æ˜¯æ˜Ÿä½³ï¼Œä¸€ä½æ·±è€•äº’è”ç½‘ã€æ•™è‚²è§„åˆ’ã€è‡ªæˆ‘æˆé•¿çš„å…¬ä¼—å·ä¸»ç†äººã€‚
ç°åœ¨ä½ éœ€è¦å†™ä¸€ç¯‡æ–°çš„å¾®ä¿¡å…¬ä¼—å·æ–‡ç« åˆç¨¿ï¼Œä¸»é¢˜æ˜¯ï¼šã€{topic}ã€‘ã€‚

è¯·ä½ å…ˆé˜…è¯»ä»¥ä¸‹ä½ è¿‡å»å†™è¿‡çš„ç›¸å…³æ–‡ç« ç‰‡æ®µï¼Œä»”ç»†ä½“ä¼šä½ è‡ªå·±çš„å†™ä½œæ–‡é£ã€æ’ç‰ˆä¹ æƒ¯ã€å¸¸ç”¨é‡‘å¥å’Œæ€ç»´é€»è¾‘ï¼š

ã€å†å²é£æ ¼å‚è€ƒã€‘ï¼š
{context_str}

ã€å†™ä½œè¦æ±‚ã€‘ï¼š
1. ä½ çš„æ–‡ç« ä¸æ˜¯å†·å†°å†°çš„ç§‘æ™®ï¼Œè€Œæ˜¯å¸¦æœ‰å¼ºçƒˆçš„ä¸ªäººç»éªŒè‰²å½©å’Œæ•…äº‹æ€§ï¼Œåƒæ˜¯åœ¨è·Ÿè¯»è€…äº¤å¿ƒã€‚
2. å®Œç¾å¤åˆ»å†å²å‚è€ƒç‰‡æ®µä¸­çš„å†™ä½œé£æ ¼ï¼ŒåŒ…æ‹¬è‡ªç„¶çš„åˆ†æ®µã€åˆç†çš„è®¾é—®ã€ä»¥åŠä¸€é’ˆè§è¡€çš„æ–­è¨€ã€‚
3. è¯·ä¸ºè¿™ç¯‡æ–‡ç« èµ·ä¸€ä¸ªå¸å¼•äººçš„ã€å¸¦æœ‰ç‚¹â€œæ˜Ÿä½³å‘³é“â€çš„æ ‡é¢˜ï¼ˆæ”¾åœ¨å…¨æ–‡æœ€å‰é¢ï¼‰ã€‚
4. å­—æ•°è¦æ±‚åœ¨ 1500 å­—ä»¥ä¸Šï¼Œç»“æ„ä¸Šè¦æœ‰ï¼šå¼•å…¥ï¼ˆå¼•èµ·å…±é¸£ï¼‰ -> ç ´é¢˜ï¼ˆç»™å‡ºç‹¬å®¶è§†è§’ï¼‰ -> è®ºè¯ï¼ˆç»“åˆå…·ä½“æ¡ˆä¾‹æˆ–é“ç†ï¼‰ -> ç»“è®ºï¼ˆä¸€å¥æœ‰åŠ›çš„ç»“è¯­ï¼‰ã€‚
5. ç›´æ¥è¾“å‡º Markdown æ ¼å¼çš„æ­£å¼æ–‡ç« å†…å®¹ï¼Œä¸éœ€è¦ä»»ä½•ä»‹ç»è‡ªå·±çš„åºŸè¯ã€‚

å¼€å§‹åˆ›ä½œå§ï¼š"""

    for chunk in client.models.generate_content_stream(
        model=chosen_model,
        contents=prompt
    ):
        if chunk.text:
            print(chunk.text, end="", flush=True)
            
    print("\n")
    print("=" * 60)
    print("\nâœ… åˆç¨¿ç”Ÿæˆå®Œæ¯•ã€‚ä½ å¯ä»¥ç›´æ¥å°† Markdown å¤åˆ¶åˆ°ç¼–è¾‘å™¨ä¸­æ¶¦è‰²ã€‚")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="AI Writer - Generate articles based on historical context.")
    parser.add_argument("-t", "--topic", type=str, required=True, help="ä½ æƒ³å†™çš„æ–‡ç« ä¸»é¢˜æˆ–å‡ ä¸ªå¤§çº²å…³é”®è¯ã€‚")
    args = parser.parse_args()
    
    generate_draft(args.topic)
