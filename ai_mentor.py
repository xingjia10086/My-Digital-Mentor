import os
from dotenv import load_dotenv
from google import genai
from langchain_google_vertexai import VertexAIEmbeddings
from langchain_community.vectorstores import Chroma

load_dotenv()

# Configuration
PROJECT_ID = os.environ.get("GCP_PROJECT_ID", "gen-lang-client-0834352502")
LOCATION = os.environ.get("GCP_LOCATION", "us-central1")
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
CHROMA_PERSIST_DIR = os.path.join(BASE_DIR, "chroma_db")
EMBEDDING_MODEL = "text-embedding-004"

# Use the new google-genai SDK with the API key
API_KEY = os.environ.get("GOOGLE_API_KEY", "")
client = genai.Client(api_key=API_KEY)


def format_docs(docs):
    formatted = []
    for d in docs:
        source = d.metadata.get('source_file', 'æœªçŸ¥æ–‡ç« ')
        formatted.append(f"ã€æ‘˜è‡ªæ–‡ç« ï¼šã€Š{source}ã€‹ã€‘\n{d.page_content}")
    return "\n\n---\n\n".join(formatted)


def main():
    print("=== åˆå§‹åŒ– æ˜Ÿä½³çš„æ•°å­—å¯¼å¸ˆå¤§è„‘ ===")
    
    if not os.path.exists(CHROMA_PERSIST_DIR):
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æœ¬åœ°çŸ¥è¯†åº“ {CHROMA_PERSIST_DIR}ã€‚è¯·å…ˆè¿è¡Œ rag_ingest.py æ„å»ºçŸ¥è¯†åº“ã€‚")
        return
    
    # Session-based Conversation History
    chat_history = [] 
    MAX_HISTORY = 3 # Remember last 3 exchanges (Q&A pairs)
    
    # Show available models first and pick the best one
    print("\n[æ£€æŸ¥å¯ç”¨çš„ Gemini æ¨¡å‹...]")
    available = []
    for m in client.models.list():
        if 'gemini' in m.name.lower() and 'generate' in str(getattr(m, 'supported_actions', '')).lower():
            available.append(m.name)
            print(f"  âœ“ {m.name}")
    
    # Pick the best available model
    chosen_model = None
    for preferred in ["gemini-2.5-pro", "gemini-2.0-flash", "gemini-1.5-pro", "gemini-1.5-flash"]:
        for a in available:
            if preferred in a:
                chosen_model = a
                break
        if chosen_model:
            break
    
    if not chosen_model and available:
        chosen_model = available[0]
    
    if not chosen_model:
        chosen_model = "gemini-2.0-flash"
        print(f"[è­¦å‘Š] æ— æ³•è‡ªåŠ¨æ£€æµ‹æ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤å€¼: {chosen_model}")
    else:
        print(f"\n[ä½¿ç”¨æ¨¡å‹]: {chosen_model}")
    
    print(f"\næ­£åœ¨åŠ è½½æœ¬åœ° ChromaDB...")
    embeddings = VertexAIEmbeddings(
        model_name=EMBEDDING_MODEL,
        project=PROJECT_ID,
        location=LOCATION
    )
    
    vectorstore = Chroma(
        persist_directory=CHROMA_PERSIST_DIR,
        embedding_function=embeddings,
        collection_name="wechat_articles"
    )
    
    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
    
    print("\nâœ… AI å¯¼å¸ˆå·²ä¸Šçº¿ã€‚è¾“å…¥ 'clear' æ¸…ç©ºè®°å¿†ï¼ŒæŒ‰ Ctrl+C é€€å‡ºã€‚\n")
    print("-" * 50)
    
    while True:
        try:
            user_input = input("\næœ‰ä»€ä¹ˆå›°æƒ‘ï¼Ÿè¯´æ¥å¬å¬ï¼š\n> ")
            if not user_input.strip():
                continue
            if user_input.lower() in ['exit', 'quit']:
                break
            if user_input.lower() == 'clear':
                chat_history = []
                print("\n[å¯¼å¸ˆ]: å¥½çš„ï¼Œæˆ‘å·²ç»æ¸…ç©ºäº†åˆšæ‰çš„å¯¹è¯è®°å¿†ï¼Œæˆ‘ä»¬é‡æ–°å¼€å§‹ã€‚")
                continue
                
            print("\n[å¯¼å¸ˆæ²‰æ€ä¸­... æ­£åœ¨æœç´¢ä½ çš„å†å²æ–‡ç« ]")
            
            # Contextualized Query: Combine current input with previous history for better retrieval
            search_query = user_input
            if chat_history:
                # Use the last Q&A to provide thematic context for the vector search
                search_query = f"å…³äº {chat_history[-1]['q']} çš„è¿›ä¸€æ­¥æ¢è®¨: {user_input}"

            # 1. Retrieve relevant history from ChromaDB
            # We increase k to 6 for a broader look, then re-rank or filter if needed
            docs = vectorstore.similarity_search(search_query, k=6) 
            context_str = format_docs(docs)
            
            print("\n[å¼•ç”¨çš„å†å²çµæ„Ÿ]:")
            unique_sources = []
            for d in docs:
                src = d.metadata.get('source_file', 'æœªçŸ¥')
                if src not in unique_sources:
                    unique_sources.append(src)
                    print(f"  ğŸ“– ã€Š{src}ã€‹")
                
            # 2. Build History String
            history_str = ""
            if chat_history:
                history_str = "\nã€æœ€è¿‘çš„å¯¹è¯å†å²ã€‘ï¼š\n"
                for h in chat_history:
                    history_str += f"æ˜Ÿä½³: {h['q']}\nå¯¼å¸ˆ: {h['a'][:200]}...\n" # Keep history summary concise
            
            # 3. Build the mentor prompt
            prompt = f"""ä½ æ˜¯æ˜Ÿä½³çš„æ•°å­—å…‹éš†ä½“ä¹Ÿæ˜¯ä»–çš„äººç”Ÿå¯¼å¸ˆã€‚ä½ æ±‡èšäº†ä»–è¿‡å»äº”å¹´çš„æ€è€ƒç»“æ™¶ä¸ç»éªŒã€‚
è¯·ç»“åˆä¸‹é¢è¿™äº›ä»–å½“å¹´å†™ä¸‹çš„å†å²æ–‡ç« ç‰‡æ®µï¼Œå¹¶èå…¥é¡¶å°–çš„å¿ƒç†å­¦ä¸å“²å­¦æˆ˜ç•¥æ€ç»´ï¼Œ
ä»¥ä¸€ä½åŒ…å®¹ä¸”å……æ»¡æ™ºæ…§çš„è‰¯å¸ˆç›Šå‹çš„å£å»ï¼Œç»“åˆå½“å‰çš„å¯¹è¯ä¸Šä¸‹æ–‡ï¼Œæ·±å…¥æ¢è®¨å¹¶è§£ç­”ä»–å½“ä¸‹çš„å›°æƒ‘ã€‚

ã€æ³¨æ„è§„åˆ™ã€‘ï¼š
1. ä½ çš„å›ç­”è¦å…·æœ‰å¯å‘æ€§ï¼Œåƒæ˜¯ä¸€ä½æ·±äº¤å¤šå¹´çš„æŒšå‹åœ¨å¯å‘ä»–æ€è€ƒã€‚
2. æ½œç§»é»˜åŒ–åœ°åŒ–ç”¨ä»–è¿‡å»å†™è¿‡çš„å¿ƒå¾—ã€æ¯”å–»ä¸é‡‘å¥ã€‚
3. å…è®¸é€‚åº¦çš„ä¸€é’ˆè§è¡€ï¼Œç”šè‡³æ˜¯æ¸©æŸ”çš„â€œå†’çŠ¯â€ï¼Œä»¥è¾¾åˆ°å¯¼å¸ˆå¼€å¯¼çš„æ•ˆæœã€‚

{history_str}

ã€å›å¿†èµ·ä½ çš„è¿‡å¾€æ€è€ƒç‰‡æ®µã€‘ï¼š
{context_str}

ã€æ˜Ÿä½³å½“å‰çš„å›°æƒ‘/è¿½é—®ã€‘ï¼š
{user_input}

ä½ çš„å›ç­”ï¼š"""

            print("\n[å¯¼å¸ˆçš„å›ç­”]:")
            full_response = ""
            # 4. Stream using the new google-genai SDK
            for chunk in client.models.generate_content_stream(
                model=chosen_model,
                contents=prompt
            ):
                if chunk.text:
                    print(chunk.text, end="", flush=True)
                    full_response += chunk.text
            print("\n")
            
            # 5. Update History
            chat_history.append({"q": user_input, "a": full_response})
            if len(chat_history) > MAX_HISTORY:
                chat_history.pop(0)

            print("-" * 50)
            
        except KeyboardInterrupt:
            print("\nå¯¼å¸ˆä¼‘æ¯äº†ã€‚å†è§ã€‚")
            break
        except Exception as e:
            print(f"\n[å‘ç”Ÿé”™è¯¯]: {e}")

if __name__ == "__main__":
    main()
